<section class="gh-content gh-canvas is-body">
  <p><strong>Gumbel AlphaZero</strong>의 강화학습을 구현해 보려고 했으나, 이론이 좀처럼 쉽지 않아서 공식 리포지토리를 참고하면서 구현해 보려고 합니다.</p>
  
  <figure class="kg-card kg-bookmark-card">
    <a href="https://github.com/google-deepmind/mctx" class="kg-bookmark-container">
      <div class="kg-bookmark-content">
        <div class="kg-bookmark-title">Mctx: MCTS-in-JAX</div>
        <div class="kg-bookmark-description">Mctx is a library with a JAX-native implementation of Monte Carlo tree search (MCTS) algorithms such as AlphaZero, MuZero, and Gumbel MuZero. For computation speed up, the implementation fully supports JIT-compilation. Search algorithms in Mctx are defined for and operate on batches of inputs, in parallel. This allows to make the most of the accelerators and enables the algorithms to work with large learned environment models parameterized by deep neural networks.</div>
        <div class="kg-bookmark-metadata">
          <img class="kg-bookmark-icon" src="<%= asset_url('octocat.svg') %>" alt="">
          <span class="kg-bookmark-author">GitHub</span>
          <span class="kg-bookmark-publisher">google-deepmind</span>
        </div>
      </div>
      <div class="kg-bookmark-thumbnail">
        <img src="https://opengraph.githubassets.com/1/google-deepmind/mctx" alt="" onerror="this.style.display='none'">
      </div>
    </a>
  </figure>

  <hr>
  
  <figure class="kg-card kg-image-card">
    <%= post_detail_image_tag("gumbel_01.webp") %>
  </figure>

  <h2>1. ⚙ Gumbel AlphaZero란?</h2>
  <p><code>Gumbel AlphaZero</code>는 기존의 <code>AlphaZero</code>가 가진 강화학습의 한계를 개선하기 위해 제안된 새로운 알고리즘입니다. 특히 적은 수의 시뮬레이션만으로도 정책(Policy)을 이론적으로 개선할 수 있다는 강력한 장점을 가집니다.</p>
  <p>이 글에서는 <code>Gumbel AlphaZero</code>를 도입하는 과정 및 그 핵심 개념과 알고리즘을 정리합니다.</p>

  <h2>2. ⚙ DeepMind Mctx: JAX 기반 MCTS 라이브러리</h2>
  <p><code>Gumbel AlphaZero</code>를 비롯한 최신 탐색 알고리즘들은 딥마인드에서 개발한 <code>Mctx</code> 라이브러리를 기반으로 구현되는 경우가 많습니다. <code>Mctx</code>는 JAX 기반으로 작성되어 Python 환경에서 높은 성능과 유연성을 제공하는 MCTS(Monte Carlo Tree Search) 라이브러리입니다.</p>

  <h3>2.1. Mctx 소개</h3>
  <p><code>Mctx</code>는 MuZero와 같은 강력한 강화학습 알고리즘을 연구자들이 더 쉽게 사용하고 발전시킬 수 있도록 돕는 것을 목표로 합니다. C++ 등으로 작성된 기존의 고성능 탐색 라이브러리와 달리, JAX를 사용하여 Python의 편의성을 유지하면서도 컴파일을 통한 성능 최적화가 가능합니다.</p>

  <h3>2.2. 환경 구축</h3>
  <p>이 글의 예제 코드를 실행하기 위해, WSL2 Ubuntu 22.04 환경에 <%= link_to "Miniconda", "https://docs.conda.io/projects/miniconda/en/latest/index.html", class: "next-post-link" %>를 설치하고 `mctx`라는 이름의 Conda 가상환경을 생성합니다.</p>
<pre class="language-bash" tabindex="0">
<code class="language-bash">
# Conda 가상환경 생성 및 활성화
conda create -n mctx python=3.11
conda activate mctx
</code>
</pre>
<p><code>visualization_demo.py</code>의 탐색 트리 시각화에 필요한 <code>Graphviz</code>와 <code>pygraphviz</code>를 설치합니다.</p>
<pre class="language-bash" tabindex="0">
<code class="language-bash">
# 1. 시스템 라이브러리 설치 (Graphviz)
sudo apt-get update && sudo apt-get install -y graphviz graphviz-dev

# 2. Conda 패키지 설치 (pygraphviz)
conda install conda-forge::pygraphviz
</code>
</pre>
<p>마지막으로 <code>JAX</code>, <code>Chex</code>, <code>Mctx</code>를 <code>pip</code>으로 설치합니다.</p>
<pre class="language-bash" tabindex="0">
<code class="language-bash">
# 3. Pip 패키지 설치

# JAX (NVIDIA GPU 환경)
pip install "jax[cuda12]"

# JAX (CPU 환경, GPU가 없는 경우)
# pip install jax

# Chex 및 Mctx
pip install chex mctx

# 또는 GitHub에서 직접 최신 개발 버전 설치
# pip install chex git+https://github.com/google-deepmind/mctx.git
</code>
</pre>

  <h2>3. ⚙ 기존 AlphaZero의 한계</h2>
  <p><code>AlphaZero</code>의 강화학습은 <strong>UCT(Upper Confidence bounds for Trees)</strong> 알고리즘을 사용하여 탐색을 수행하고, 그 결과(방문 횟수)를 바탕으로 정책을 개선합니다. 하지만 이 방식은 시뮬레이션 횟수가 충분히 많지 않을 경우, 학습이 우연히 샘플링된 행동에만 의존하게 되어 정책 개선을 이론적으로 보장할 수 없는 문제가 있었습니다.</p>
  <p>이는 곧, 제한된 시간 안에 최적의 수를 찾아야 하는 실제 대국 환경에서 약점으로 작용할 수 있습니다.</p>

  <h2>4. ⚙ Gumbel AlphaZero의 혁신</h2>
  <p><code>Gumbel AlphaZero</code>는 루트 노드(탐색의 시작점)에서 PUCB(Polynomial Upper Confidence Bound) 대신 <strong>Gumbel-Top-k</strong>라는 새로운 기법을 사용합니다. 이를 통해 더 적은 시뮬레이션으로도 정책이 개선될 것임을 이론적으로 보장합니다.</p>
  <p>또한, 탐색 과정에서 <strong>순차적 반감법(Sequential Halving)</strong>을 함께 사용하여 제한된 시뮬레이션 예산을 가장 유망한 후보 수에 효율적으로 배분합니다. 이 두 가지 핵심적인 변화를 통해 <code>Gumbel AlphaZero</code>는 기존 <code>AlphaZero</code>보다 효율적이고 안정적인 학습을 가능하게 합니다.</p>

  <h2>5. ⚙ <code>visualization_demo.py</code>로 이해하는 탐색 알고리즘</h2>
  <p><code>Mctx</code> 공식 저장소에는 <code>Gumbel AlphaZero</code>의 동작을 시각적으로 보여주는 <code>visualization_demo.py</code> 예제가 포함되어 있습니다. 이 코드는 <code>Mctx</code>의 핵심 정책 중 하나인 <code>gumbel_muzero_policy</code>를 사용하여 탐색을 수행합니다.</p>
  <p><code>visualization_demo.py</code>를 실행하면 탐색 과정이 시각적으로 보여지며, 탐색 결과는 `/tmp/search_tree.png`에 저장됩니다.</p>
  <p>출력 예시:</p>
<pre class="language-plain" tabindex="0">
<code class="language-plain">
Starting search.
Selected action: 1
Selected action Q-value: 10.666667
Saving tree diagram to: /tmp/search_tree.png
</code>
</pre>
  <figure class="kg-card kg-image-card">
    <%= post_detail_image_tag("search_tree.png") %>
  </figure>

  <h3>5.1. Mctx의 핵심 구성 요소</h3>
  <p><code>Mctx</code>의 정책 함수를 사용하기 위해서는 사용자가 몇 가지 구성 요소를 직접 정의하여 제공해야 합니다. 이는 <code>Mctx</code>가 특정 게임이나 환경에 종속되지 않고 범용적으로 사용될 수 있도록 설계되었기 때문입니다.</p>
  <ul>
    <li><code><strong>RootFnOutput</strong></code>: 탐색을 시작하는 루트 노드의 상태를 나타냅니다. 정책 네트워크가 출력하는 <strong>정책 확률(prior_logits)</strong>, <strong>상태 가치(value)</strong>, 그리고 상태를 표현하는 <strong>임베딩(embedding)</strong>을 포함합니다.</li>
    <li><code><strong>recurrent_fn</strong></code>: 환경의 동역학 모델 역할을 하는 함수입니다. 현재 상태의 <code>embedding</code>과 선택된 <code>action</code>을 입력받아, 다음 상태의 <code>embedding</code>과 함께 전이 과정에서 얻는 <strong>보상(reward)</strong>, <strong>할인율(discount)</strong>, 그리고 다음 상태의 <strong>정책 확률</strong>과 <strong>가치</strong>를 반환합니다.</li>
  </ul>

  <h3>5.2. 환경 정의</h3>
  <p>상태 전이와 보상은 아래와 같이 정의되어 있습니다.</p>
<pre class="language-python" tabindex="0">
<code class="language-python">
# We will define a deterministic toy environment.
# The deterministic `transition_matrix` has shape `[num_states, num_actions]`.
# The `transition_matrix[s, a]` holds the next state.
transition_matrix = jnp.array([
    [1, 2, 3, 4],
    [0, 5, 0, 0],
    [0, 0, 0, 6],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
], dtype=jnp.int32)
# The `rewards` have shape `[num_states, num_actions]`. The `rewards[s, a]`
# holds the reward for that (s, a) pair.
rewards = jnp.array([
    [1, -1, 0, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
    [10, 0, 20, 0],
], dtype=jnp.float32)
</code>
</pre>

  <p>총 7개의 상태가 있으며, <code>transition_matrix</code>는 상태 전이를, <code>rewards</code>는 각 행동에 대한 보상을 정의합니다.</p>
  <p>또한 상태 가치에 대한 할인율도 정의되어 있습니다.</p>
<pre class="language-python" tabindex="0">
<code class="language-python">
# The discount for each (s, a) pair.
discounts = jnp.where(transition_matrix > 0, 1.0, 0.0)
</code>
</pre>

  <p><code>transition_matrix</code>의 값이 0, 즉 다음 상태가 없는 경우 할인율이 0이 됩니다.</p>

  <h3>5.3. 상태 가치의 초기값</h3>
  <p>각 상태의 초기 가치는 15로 설정되어 있습니다. 이는 탐색 초기에 모든 상태를 낙관적으로 평가하여 탐험(exploration)을 장려하기 위함입니다.</p>
<pre class="language-python" tabindex="0">
<code class="language-python">
# Using optimistic initial values to encourage exploration.
values = jnp.full([num_states], 15.0)
</code>
</pre>

  <h3>5.4. 정책 네트워크의 사전 확률</h3>
  <p>각 상태에서 정책 네트워크가 출력하는 사전 확률의 logits은 0으로 설정되어 있습니다. 이는 탐색 초기 단계에서 모든 행동이 동일한 확률을 갖도록 합니다.</p>
<pre class="language-python" tabindex="0">
<code class="language-python">
# The prior policies for each state.
all_prior_logits = jnp.zeros_like(rewards)
</code>
</pre>

  <h3>5.5. 루트 노드와 상태 전이 함수 호출 예시</h3>
  <p><code>_make_batched_env_model</code> 함수를 사용하여 루트 노드와 상태 전이 함수를 정의합니다.</p>
<pre class="language-python" tabindex="0">
<code class="language-python">
def _make_batched_env_model(
    batch_size: int,
    *,
    transition_matrix: chex.Array,
    rewards: chex.Array,
    discounts: chex.Array,
    values: chex.Array,
    prior_logits: chex.Array):
  """Returns a batched `(root, recurrent_fn)`."""
  chex.assert_equal_shape([transition_matrix, rewards, discounts,
                           prior_logits])
  num_states, num_actions = transition_matrix.shape
  chex.assert_shape(values, [num_states])
  # We will start the search at state zero.
  root_state = 0
  root = mctx.RootFnOutput(
      prior_logits=jnp.full([batch_size, num_actions],
                            prior_logits[root_state]),
      value=jnp.full([batch_size], values[root_state]),
      # The embedding will hold the state index.
      embedding=jnp.zeros([batch_size], dtype=jnp.int32),
  )

  def recurrent_fn(params, rng_key, action, embedding):
    del params, rng_key
    chex.assert_shape(action, [batch_size])
    chex.assert_shape(embedding, [batch_size])
    recurrent_fn_output = mctx.RecurrentFnOutput(
        reward=rewards[embedding, action],
        discount=discounts[embedding, action],
        prior_logits=prior_logits[embedding],
        value=values[embedding])
    next_embedding = transition_matrix[embedding, action]
    return recurrent_fn_output, next_embedding

  return root, recurrent_fn
</code>
</pre>
<pre class="language-python" tabindex="0">
<code class="language-python">
root, recurrent_fn = _make_batched_env_model(
    # Using batch_size=2 to test the batched search.
    batch_size=2,
    transition_matrix=transition_matrix,
    rewards=rewards,
    discounts=discounts,
    values=values,
    prior_logits=all_prior_logits
)
</code>
</pre>

  <p>루트 노드는 <code>RootFnOutput</code> 타입으로, <code>사전 확률</code>, <code>가치</code>, <code>상태 임베딩</code>을 보유합니다.</p>
  <p>MuZero에서는 이 임베딩이 상태를 표현하는 벡터가 되지만, 이 예제에서는 상태의 인덱스를 그대로 사용합니다.</p>
  <p>상태 전이 함수 <code>recurrent_fn</code>은 현재 상태의 <code>embedding</code>과 선택된 <code>action</code>을 입력받아, 전이 과정에서 얻는 정보(<code>RecurrentFnOutput</code>)와 다음 상태의 <code>embedding</code>을 반환합니다.</p>
  <p>배치 단위 탐색을 위해 각 데이터의 첫 번째 차원은 배치 크기가 됩니다.</p>

  <h3>5.6. <code>gumbel_muzero_policy</code> 호출 예시</h3>
  <p><code>visualization_demo.py</code>에서는 위에서 정의한 구성 요소들을 <code>gumbel_muzero_policy</code>에 전달하여 탐색을 수행합니다.</p>
<pre class="language-python" tabindex="0">
<code class="language-python">
# Running the search.
policy_output = mctx.gumbel_muzero_policy(
    params=(),
    rng_key=rng_key,
    root=root,
    recurrent_fn=recurrent_fn,
    num_simulations=FLAGS.num_simulations,
    max_depth=FLAGS.max_depth,
    max_num_considered_actions=FLAGS.max_num_considered_actions,
)
</code>
</pre>

  <p>인수는 <code>rng_key</code>, 루트 노드(<code>root</code>), 상태 전이 함수(<code>recurrent_fn</code>), 시뮬레이션 횟수(<code>num_simulations</code>), 탐색 깊이(<code>max_depth</code>), 루트 노드의 최대 행동 수(<code>max_num_considered_actions</code>)입니다.</p>
  <p>참고로 <code>rng_key</code>는 난수를 생성하기 위해 사용됩니다.</p>
  <p>탐색이 완료되면 <code>policy_output</code> 객체에 결과가 담겨 반환됩니다. <code>policy_output.action</code>은 탐색을 통해 결정된 최적의 행동을, <code>policy_output.action_weights</code>는 정책 네트워크 학습에 사용될 수 있는 목표 확률값을 담고 있습니다.</p>

  <h2>6. 🏁 마치며</h2>
  <p>이번 포스트에서는 <code>Gumbel AlphaZero</code>의 기본 개념과 그 기반이 되는 <code>Mctx</code> 라이브러리에 대해 알아보았습니다. <code>Mctx</code>의 설치 방법과 <code>visualization_demo.py</code> 예제를 통해 탐색 알고리즘이 어떤 구성 요소들을 필요로 하고 어떻게 동작하는지 살펴보았습니다.</p>
  <p><%= link_to "다음 포스트", post_by_slug_path("2025-09-04-gumbel-alphazero-02"), class: "next-post-link" %>에서는 <code>Gumbel AlphaZero</code>의 핵심이라 할 수 있는 <strong>행동 선택(Action Selection)</strong> 알고리즘, 특히 <code>Gumbel-Top-k</code>와 <code>순차적 반감법</code>에 대해 더 자세히 알아보겠습니다.</p>
</section>
